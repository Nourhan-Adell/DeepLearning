{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMY778MM/ExQLCBiNVXmNp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nourhan-Adell/DeepLearning/blob/main/Tokenize_Basics(Out__of_vocabulary_problem).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenize Basics (Out of vocabulary problem)**\n",
        "\n",
        "**Steps:**\n",
        "1. Encode the sentence(word_idex)\n",
        "2. Turn the sentences into list of values based on these tokens\n",
        "3. Manipulate these lists(not least to make every sentence the same lenght)\n"
      ],
      "metadata": {
        "id": "-cg_NsChir4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import libraries**"
      ],
      "metadata": {
        "id": "Hs0spWCHepS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "ySv-fpNIesUJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer API**: \n",
        "- Will handle the heavy lifting for us, by generating the dictionary of word encodings and creating vectors out of the sentences\n",
        "- It also remove the punctuation from the sentences\n",
        "- It changes all the upper characters into lower one."
      ],
      "metadata": {
        "id": "CpY-P5UzfZ5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Start building the model**"
      ],
      "metadata": {
        "id": "X7agnfrwe6vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input sentences \n",
        "sentences = ['I love my dog',\n",
        "             'I love my cat',\n",
        "             'You love my dog!',\n",
        "             'Do you think my dog is amazing?']"
      ],
      "metadata": {
        "id": "SKvk_LGFe5DF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intialize the tokenizer class\n",
        "tokenizer = Tokenizer(num_words= 100)   \n",
        "\n",
        "#Generate indecies for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)  \n",
        "\n",
        "# Get the indecies and print them\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "#\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(word_index)\n",
        "print()\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ4UREMOfLrT",
        "outputId": "e2dac1cb-1f6d-476f-be0f-67743c198393"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "\n",
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the **texts_to_sequences** function is convertig each sentence to a list according to the word encode"
      ],
      "metadata": {
        "id": "jldojv6klAvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = ['I really love my dog',\n",
        "             'My dog loves my manatee']\n",
        "\n",
        "test_seq =  tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGwgzkZ-k4gO",
        "outputId": "680cd5ac-4555-4f71-9af4-99d90a3346ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 2, 1, 3], [1, 3, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above test data, We can coclude that the lists of the encoded test data doesn't include the new data so, it incodes the first sentence as it is (I love my dog) \n",
        "\n",
        "**So, It has ignored the unseen word in the test data**"
      ],
      "metadata": {
        "id": "j1XNXTVOl9qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M1YWA9OAmute"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adding new property to solve ignoring words problem**"
      ],
      "metadata": {
        "id": "r6ehMXIqmu1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intialize the tokenizer class\n",
        "tokenizer = Tokenizer(num_words= 100, oov_token= '<OOV>')     #oov: out of vocabulary   \n",
        "\n",
        "#Generate indecies for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)  \n",
        "\n",
        "# Get the indecies and print them\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "#\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(word_index)\n",
        "print()\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzV1hQ-Nl8ii",
        "outputId": "61660cca-7261-4432-a80a-a7cf9ab9ef30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq =  tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsMTraFhnFV3",
        "outputId": "4570ebe8-da23-4238-de97-78990086752b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here we replace each unseen word into 'oov' instead of removing it totally."
      ],
      "metadata": {
        "id": "5sV4aCZQniKV"
      }
    }
  ]
}